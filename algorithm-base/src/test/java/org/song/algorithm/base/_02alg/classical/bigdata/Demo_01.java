package org.song.algorithm.base._02alg.classical.bigdata;

/**
 * 大数据量处理的一般算法
 * 摘自书籍<程序员代码面试指南> 左程云
 */
public class Demo_01 {
    /*
    第一题: 有一个包含20亿个全是32位整数的大文件, 在其中找到出现次数最多的数, 内存限制为2GB
    
    思路 1
    使用 HashMap
        空间计算 
            key = 4byte * 20亿 = 7.5G
            val = 4byte * 20亿 = 7.5G
         单个节点空间会超出, 所以至少采用4台单节点2G的节点机器处理
         
     思路 2 推荐 分而治之
     将20亿个数采用Hash算法分在16个文件中, (具体多少个文件, 可以根据数据量内存限制大小等条件动态配置)
     由于Hash函数的特性, 相同的数字肯定会分配到同一个文件中, 所以只需要单独计算出16个文件次数最多的数最后选出一个最多的即可
     */
    
    /*
    第二题: 40亿个非负整数中找到未出现的数
    32位无符号整数的范围是0~4 294 967 295, 现在有一个正好包含40亿个无符号整数的文件, 所以在整个范围中必然有未出现过的数. 可以使用最多1GB的内存
    进阶: 内存限制为10MB, 但是只用找到一个没出现过的数即可
    
    思路 1
    暴力算法, 既然目标数字在指定范围内, 就那指定范围内的数一个一个尝试对比匹配, 找到没有匹配到的数, 就是未出现的数;
        先从1开始, 在目标数据中寻找, 然后从2开始....
        时间复杂度O(n^2) 不满足
        空间复杂度取决于一次读取多少个数O(1) 最小1byte 满足
        
    思路 2 推荐
    采用bitmap存储已出现的数, 然后通过bitmap找出为出现的数
        时间复杂度 O(n); 遍历所有数据存入bitmap, 遍历bitmap取出未出现的数字
        空间分析, bitmap大小 = 40亿位 = 477M 满足基础版要求
        
    思路 3 推荐 分而治之
    将 4 294 967 295 这些数据等分成64个区间, 每个区间的数的边界也确定下来了, 每个区间的数的数量也确定下来, 正好是 671 088 64
        1. 准备一个64长度的整数数组
            遍历40亿的数, 将满足第一个区间的数条件的数的数量存在数组中, 
            当把所有数都遍历计算完成后, 就得到了那个区间数量小于671 088 64, 则那个区间肯定有没出现的数, 前提是不能存在重复的数
        2. 找到那个区间的数的范围进行二次处理, 两种方式, 
            1. 还是采用类似的思路, 再次分区, 将目标结果范围进一步缩小
            2. 此时的数据量只有 671 088 64, 采用思路2中的位图则空间大小仅仅有8M, 满足要求
     */
    
    /*
    第三题: 找到100亿个URL中重复的URL及搜索词汇的TopK问题
    有一个包含100亿个URL的大文件, 假设每个URL占用64B, 请找出其中所有重复的URL. 
    补充问题: 某搜索公司一天的用户搜索词汇是海量的(百亿数据量), 请设计一种求出每天热门Top 100词汇的可行办法. 
    
    思路 1
    先计算出每个URL的出现的次数, 然后取TopK
        计算次数采用HashMap, key=URL, val=次数
            时间复杂度 O(n); n=40亿
            空间复杂度 O(n); 100亿 * 64B = 600G
        TopK采用小堆, 堆得大小就是k, 遍历HashMap的val, 将更大的数替换进堆中, 遍历完成后就得到了TopK
            时间复杂度 O(n*k), n=k
            空间复杂度 O(n), n=k
            取最大用小堆, 取最小用大堆
    
    思路 2 推荐 分而治之
    将大文件通过hash分成多个小文件(机器), 然后在对小文件逐一计算, 由于Hash函数的特性, 相同的URL会被分配到相同的文件中, 所以单个URL计算次数肯定是准确的
        文件(机器)数量是64, 则单次空间降低到10G
        文件(机器)数量是100, 则单次空间降低到6G
        
        针对单个文件进行TopK计算, 注意此时的K依然是K并不需要进行拆分, 因为需要将结果合并
     */
}
